{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9713f3",
   "metadata": {},
   "source": [
    "Monitorear el entrenamiento con tensorboard (tensorboard --logdir=runs) en el directorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7d5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pycocotools.coco import COCO\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1be0980b",
   "metadata": {},
   "source": [
    "# Ensure the directory for the data exists\n",
    "if not os.path.exists('COCO'):\n",
    "    os.makedirs('COCO')\n",
    "\n",
    "last_progress = -1\n",
    "\n",
    "def show_progress(block_num, block_size, total_size):\n",
    "    global last_progress\n",
    "    downloaded = block_num * block_size\n",
    "    progress = int(downloaded * 100 / total_size)\n",
    "    if progress != last_progress:\n",
    "        print(f'Download progress: {progress}%')\n",
    "        last_progress = progress\n",
    "\n",
    "\n",
    "# List of files to download\n",
    "files_to_download = ['http://images.cocodataset.org/zips/train2017.zip',\n",
    "                     'http://images.cocodataset.org/zips/val2017.zip',\n",
    "                     'http://images.cocodataset.org/annotations/annotations_trainval2017.zip']\n",
    "\n",
    "for file_url in files_to_download:\n",
    "    # Download the file\n",
    "    print(f'Downloading {file_url}')\n",
    "    file_name = file_url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(file_url, file_name, show_progress)\n",
    "\n",
    "    # Unzip the file\n",
    "    print('Unzipping file...')\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall('COCO')\n",
    "\n",
    "    # Remove the downloaded zip file\n",
    "    os.remove(file_name)\n",
    "\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a932f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        return self.transform(image), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d493593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502b902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def targets_od(images, targets_tuple, device):\n",
    "    TARGETS = []\n",
    "    IMAGES = []\n",
    "    for img, t_list in zip(images, targets_tuple):\n",
    "        bbox = []\n",
    "        category_id = []\n",
    "        d = {}\n",
    "        for t_dict in t_list:\n",
    "            if list(torch.tensor(t_dict.get('bbox', [])).to(device).size()) != [0]:  # Check the size of bbox\n",
    "                for k, v in t_dict.items():\n",
    "                    if k == 'bbox':\n",
    "                        # Convert box from [x, y, width, height] to [x1, y1, x2, y2]\n",
    "                        converted_box = [v[0], v[1], v[0] + v[2], v[1] + v[3]]\n",
    "                        bbox.append(converted_box)\n",
    "                    elif k == 'category_id':\n",
    "                        category_id.append(v)\n",
    "        if bbox:  # if bbox list is not empty\n",
    "            d['boxes'] = torch.tensor(bbox).to(device)\n",
    "            d['labels'] = torch.tensor(category_id).to(device)\n",
    "            TARGETS.append(d)\n",
    "            IMAGES.append(img.to(device))\n",
    "        else:\n",
    "            if len(TARGETS) > 0:  # if there is any previous image in the batch\n",
    "                TARGETS.append(TARGETS[-1])  # repeat the last valid target\n",
    "                IMAGES.append(IMAGES[-1])  # repeat the last valid image\n",
    "    return IMAGES, TARGETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4bff08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.42s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\one2\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "C:\\Users\\User\\anaconda3\\envs\\one2\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, starting from scratch.\n",
      "[1,   100] loss: 1.687\n",
      "[1,   200] loss: 0.634\n",
      "[1,   300] loss: 0.396\n",
      "[1,   400] loss: 0.260\n",
      "[1,   500] loss: 0.208\n",
      "[1,   600] loss: 0.162\n",
      "[1,   700] loss: 0.152\n",
      "[1,   800] loss: 0.123\n",
      "[1,   900] loss: 0.108\n",
      "[1,  1000] loss: 0.095\n",
      "[1,  1100] loss: 0.090\n",
      "[1,  1200] loss: 0.085\n",
      "[1,  1300] loss: 0.079\n",
      "[1,  1400] loss: 0.072\n",
      "[1,  1500] loss: 0.070\n",
      "[1,  1600] loss: 0.063\n",
      "[1,  1700] loss: 0.060\n",
      "[1,  1800] loss: 0.050\n",
      "[1,  1900] loss: 0.050\n",
      "[1,  2000] loss: 0.047\n",
      "[1,  2100] loss: 0.044\n",
      "[1,  2200] loss: 0.044\n",
      "[1,  2300] loss: 0.043\n",
      "[1,  2400] loss: 0.040\n",
      "[1,  2500] loss: 0.040\n",
      "[1,  2600] loss: 0.040\n",
      "[1,  2700] loss: 0.037\n",
      "[1,  2800] loss: 0.035\n",
      "[1,  2900] loss: 0.035\n",
      "[1,  3000] loss: 0.033\n",
      "[1,  3100] loss: 0.030\n",
      "[1,  3200] loss: 0.029\n",
      "[1,  3300] loss: 0.030\n",
      "[1,  3400] loss: 0.029\n",
      "[1,  3500] loss: 0.028\n",
      "[1,  3600] loss: 0.028\n",
      "[1,  3700] loss: 0.028\n",
      "[1,  3800] loss: 0.027\n",
      "[1,  3900] loss: 0.025\n",
      "[1,  4000] loss: 0.024\n",
      "[1,  4100] loss: 0.024\n",
      "[1,  4200] loss: 0.023\n",
      "[1,  4300] loss: 0.024\n",
      "[1,  4400] loss: 0.022\n",
      "[1,  4500] loss: 0.021\n",
      "[1,  4600] loss: 0.021\n",
      "[1,  4700] loss: 0.021\n",
      "[1,  4800] loss: 0.021\n",
      "[1,  4900] loss: 0.020\n",
      "[1,  5000] loss: 0.022\n",
      "[1,  5100] loss: 0.020\n",
      "[1,  5200] loss: 0.019\n",
      "[1,  5300] loss: 0.019\n",
      "[1,  5400] loss: 0.018\n",
      "[1,  5500] loss: 0.019\n",
      "[1,  5600] loss: 0.018\n",
      "[1,  5700] loss: 0.017\n",
      "[1,  5800] loss: 0.017\n",
      "[1,  5900] loss: 0.017\n",
      "[1,  6000] loss: 0.016\n",
      "[1,  6100] loss: 0.016\n",
      "[1,  6200] loss: 0.016\n",
      "[1,  6300] loss: 0.015\n",
      "[1,  6400] loss: 0.015\n",
      "[1,  6500] loss: 0.015\n",
      "[1,  6600] loss: 0.015\n",
      "[1,  6700] loss: 0.015\n",
      "[1,  6800] loss: 0.014\n",
      "[1,  6900] loss: 0.014\n",
      "[1,  7000] loss: 0.015\n",
      "[1,  7100] loss: 0.013\n",
      "[1,  7200] loss: 0.014\n",
      "[1,  7300] loss: 0.014\n",
      "[1,  7400] loss: 0.013\n",
      "[1,  7500] loss: 0.013\n",
      "[1,  7600] loss: 0.014\n",
      "[1,  7700] loss: 0.013\n",
      "[1,  7800] loss: 0.013\n",
      "[1,  7900] loss: 0.013\n",
      "[1,  8000] loss: 0.011\n",
      "[1,  8100] loss: 0.011\n",
      "[1,  8200] loss: 0.012\n",
      "[1,  8300] loss: 0.011\n",
      "[1,  8400] loss: 0.012\n",
      "[1,  8500] loss: 0.012\n",
      "[1,  8600] loss: 0.012\n",
      "[1,  8700] loss: 0.011\n",
      "[1,  8800] loss: 0.011\n",
      "[1,  8900] loss: 0.011\n",
      "[1,  9000] loss: 0.010\n",
      "[1,  9100] loss: 0.010\n",
      "[1,  9200] loss: 0.011\n",
      "[1,  9300] loss: 0.010\n",
      "[1,  9400] loss: 0.010\n",
      "[1,  9500] loss: 0.010\n",
      "[1,  9600] loss: 0.010\n",
      "[1,  9700] loss: 0.010\n",
      "[1,  9800] loss: 0.010\n",
      "[1,  9900] loss: 0.010\n",
      "[1, 10000] loss: 0.010\n",
      "[1, 10100] loss: 0.009\n",
      "[1, 10200] loss: 0.009\n",
      "[1, 10300] loss: 0.009\n",
      "[1, 10400] loss: 0.010\n",
      "[1, 10500] loss: 0.009\n",
      "[1, 10600] loss: 0.009\n",
      "[1, 10700] loss: 0.009\n",
      "[1, 10800] loss: 0.009\n",
      "[1, 10900] loss: 0.008\n",
      "[1, 11000] loss: 0.009\n",
      "[1, 11100] loss: 0.008\n",
      "[1, 11200] loss: 0.008\n",
      "[1, 11300] loss: 0.009\n",
      "[1, 11400] loss: 0.008\n",
      "[1, 11500] loss: 0.008\n",
      "[1, 11600] loss: 0.008\n",
      "[1, 11700] loss: 0.008\n",
      "[1, 11800] loss: 0.008\n",
      "[1, 11900] loss: 0.008\n",
      "[1, 12000] loss: 0.008\n",
      "[1, 12100] loss: 0.008\n",
      "[1, 12200] loss: 0.008\n",
      "[1, 12300] loss: 0.008\n",
      "[1, 12400] loss: 0.008\n",
      "[1, 12500] loss: 0.008\n",
      "[1, 12600] loss: 0.008\n",
      "[1, 12700] loss: 0.007\n",
      "[1, 12800] loss: 0.007\n",
      "[1, 12900] loss: 0.007\n",
      "[1, 13000] loss: 0.007\n",
      "[1, 13100] loss: 0.007\n",
      "[1, 13200] loss: 0.007\n",
      "[1, 13300] loss: 0.007\n",
      "[1, 13400] loss: 0.007\n",
      "[1, 13500] loss: 0.007\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "All bounding boxes should have positive height and width. Found invalid box [296.6499938964844, 388.3299865722656, 297.67999267578125, 388.3299865722656] for target at index 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25980\\3066087350.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25980\\3066087350.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\one2\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\one2\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     95\u001b[0m                     torch._assert(\n\u001b[0;32m     96\u001b[0m                         \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                         \u001b[1;34m\"All bounding boxes should have positive height and width.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                         \u001b[1;34mf\" Found invalid box {degen_bb} for target at index {target_idx}.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\one2\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m    851\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    852\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 853\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    854\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: All bounding boxes should have positive height and width. Found invalid box [296.6499938964844, 388.3299865722656, 297.67999267578125, 388.3299865722656] for target at index 1."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    writer = SummaryWriter('runs/experiment_1')\n",
    "    transform = MyTransform(transforms.Compose([\n",
    "                transforms.Resize((800,800)),\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ]))\n",
    "\n",
    "\n",
    "    # Load the COCO dataset\n",
    "    train_dataset = CocoDetection(root='COCO/train2017',\n",
    "                                  annFile='COCO/annotations/instances_train2017.json', transforms=transform)\n",
    "    val_dataset = CocoDetection(root='COCO/val2017',\n",
    "                                annFile='COCO/annotations/instances_val2017.json', transforms=transform)\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=3, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Define the model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Move model to gpu if available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Set the optimizer and the loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.00001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Checkpoint saving path\n",
    "    ckpt_path = 'checkpoints/model_ckpt.pt'\n",
    "    start_epoch = 0\n",
    "    # Load the checkpoint if it exists\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f'Loaded checkpoint from epoch {start_epoch}')\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "\n",
    "    # Entrenar la red y guardar las pérdidas para la visualización\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = 15\n",
    "\n",
    "    for epoch in range(start_epoch, 4):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            images, targets = targets_od(images, targets, device)\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            total_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += total_loss\n",
    "            if i % 100 == 99 or i == len(train_loader) - 1:  # Asume que la indexación comienza en 0\n",
    "                avg_loss = running_loss / (i+1)\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n",
    "                train_losses.append(avg_loss)\n",
    "                writer.add_scalar('training loss', avg_loss, epoch * len(train_loader) + i)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images, targets = targets_od(images, targets, device)\n",
    "\n",
    "                loss_dict = model(images, targets)\n",
    "                total_vloss = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += total_vloss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                # Use IOU or some other suitable metric for object detection\n",
    "                # ...\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Validation loss: {avg_val_loss:.3f}')\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        writer.add_scalar('validation loss', avg_val_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, f'checkpoints/checkpoint{epoch}.pth')\n",
    "            print(f'Saved checkpoint at epoch {epoch}')\n",
    "\n",
    "        # Save the last checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, 'checkpoints/latest_checkpoint.pth')\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print('Early stopping!')\n",
    "                model.load_state_dict(torch.load('best_model.pth'))\n",
    "                break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    writer.close()\n",
    "     \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "one2",
   "language": "python",
   "name": "one2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
