{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7d5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pycocotools.coco import COCO\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1be0980b",
   "metadata": {},
   "source": [
    "# Ensure the directory for the data exists\n",
    "if not os.path.exists('COCO'):\n",
    "    os.makedirs('COCO')\n",
    "\n",
    "last_progress = -1\n",
    "\n",
    "def show_progress(block_num, block_size, total_size):\n",
    "    global last_progress\n",
    "    downloaded = block_num * block_size\n",
    "    progress = int(downloaded * 100 / total_size)\n",
    "    if progress != last_progress:\n",
    "        print(f'Download progress: {progress}%')\n",
    "        last_progress = progress\n",
    "\n",
    "\n",
    "# List of files to download\n",
    "files_to_download = ['http://images.cocodataset.org/zips/train2017.zip',\n",
    "                     'http://images.cocodataset.org/zips/val2017.zip',\n",
    "                     'http://images.cocodataset.org/annotations/annotations_trainval2017.zip']\n",
    "\n",
    "for file_url in files_to_download:\n",
    "    # Download the file\n",
    "    print(f'Downloading {file_url}')\n",
    "    file_name = file_url.split('/')[-1]\n",
    "    urllib.request.urlretrieve(file_url, file_name, show_progress)\n",
    "\n",
    "    # Unzip the file\n",
    "    print('Unzipping file...')\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall('COCO')\n",
    "\n",
    "    # Remove the downloaded zip file\n",
    "    os.remove(file_name)\n",
    "\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "38224895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.87s)\n",
      "creating index...\n",
      "index created!\n",
      "0 person\n",
      "1 bicycle\n",
      "2 car\n",
      "3 motorcycle\n",
      "4 airplane\n",
      "5 bus\n",
      "6 train\n",
      "7 truck\n",
      "8 boat\n",
      "9 traffic light\n",
      "10 fire hydrant\n",
      "11 stop sign\n",
      "12 parking meter\n",
      "13 bench\n",
      "14 bird\n",
      "15 cat\n",
      "16 dog\n",
      "17 horse\n",
      "18 sheep\n",
      "19 cow\n",
      "20 elephant\n",
      "21 bear\n",
      "22 zebra\n",
      "23 giraffe\n",
      "24 backpack\n",
      "25 umbrella\n",
      "26 handbag\n",
      "27 tie\n",
      "28 suitcase\n",
      "29 frisbee\n",
      "30 skis\n",
      "31 snowboard\n",
      "32 sports ball\n",
      "33 kite\n",
      "34 baseball bat\n",
      "35 baseball glove\n",
      "36 skateboard\n",
      "37 surfboard\n",
      "38 tennis racket\n",
      "39 bottle\n",
      "40 wine glass\n",
      "41 cup\n",
      "42 fork\n",
      "43 knife\n",
      "44 spoon\n",
      "45 bowl\n",
      "46 banana\n",
      "47 apple\n",
      "48 sandwich\n",
      "49 orange\n",
      "50 broccoli\n",
      "51 carrot\n",
      "52 hot dog\n",
      "53 pizza\n",
      "54 donut\n",
      "55 cake\n",
      "56 chair\n",
      "57 couch\n",
      "58 potted plant\n",
      "59 bed\n",
      "60 dining table\n",
      "61 toilet\n",
      "62 tv\n",
      "63 laptop\n",
      "64 mouse\n",
      "65 remote\n",
      "66 keyboard\n",
      "67 cell phone\n",
      "68 microwave\n",
      "69 oven\n",
      "70 toaster\n",
      "71 sink\n",
      "72 refrigerator\n",
      "73 book\n",
      "74 clock\n",
      "75 vase\n",
      "76 scissors\n",
      "77 teddy bear\n",
      "78 hair drier\n",
      "79 toothbrush\n"
     ]
    }
   ],
   "source": [
    "# Carga las anotaciones de COCO\n",
    "coco = COCO('COCO/annotations/instances_train2017.json')\n",
    "\n",
    "# Obtén todas las categorías\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "\n",
    "# Imprime los nombres de todas las categorías\n",
    "for i, cat in enumerate(cats):\n",
    "    print(i, cat['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5a932f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        return self.transform(image), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9d493593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "efc409bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.20s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "No checkpoint found, starting from scratch.\n",
      "[{'segmentation': [[500.49, 473.53, 599.73, 419.6, 612.67, 375.37, 608.36, 354.88, 528.54, 269.66, 457.35, 201.71, 420.67, 187.69, 389.39, 192.0, 19.42, 360.27, 1.08, 389.39, 2.16, 427.15, 20.49, 473.53]], 'area': 120057.13925, 'iscrowd': 0, 'image_id': 9, 'bbox': [1.08, 187.69, 611.59, 285.84], 'category_id': 51, 'id': 1038967}, {'segmentation': [[357.03, 69.03, 311.73, 15.1, 550.11, 4.31, 631.01, 62.56, 629.93, 88.45, 595.42, 185.53, 513.44, 230.83, 488.63, 232.99, 437.93, 190.92, 429.3, 189.84, 434.7, 148.85, 410.97, 121.89, 359.19, 74.43, 358.11, 65.8]], 'area': 44434.751099999994, 'iscrowd': 0, 'image_id': 9, 'bbox': [311.73, 4.31, 319.28, 228.68], 'category_id': 51, 'id': 1039564}, {'segmentation': [[249.6, 348.99, 267.67, 311.72, 291.39, 294.78, 304.94, 294.78, 326.4, 283.48, 345.6, 273.32, 368.19, 269.93, 385.13, 268.8, 388.52, 257.51, 393.04, 250.73, 407.72, 240.56, 425.79, 230.4, 441.6, 229.27, 447.25, 237.18, 447.25, 256.38, 456.28, 254.12, 475.48, 263.15, 486.78, 271.06, 495.81, 264.28, 498.07, 257.51, 500.33, 255.25, 507.11, 259.76, 513.88, 266.54, 513.88, 273.32, 513.88, 276.71, 526.31, 276.71, 526.31, 286.87, 519.53, 291.39, 519.53, 297.04, 524.05, 306.07, 525.18, 315.11, 529.69, 329.79, 529.69, 337.69, 530.82, 348.99, 536.47, 339.95, 545.51, 350.12, 555.67, 360.28, 557.93, 380.61, 561.32, 394.16, 565.84, 413.36, 522.92, 441.6, 469.84, 468.71, 455.15, 474.35, 307.2, 474.35, 316.24, 464.19, 330.92, 438.21, 325.27, 399.81, 310.59, 378.35, 301.55, 371.58, 252.99, 350.12]], 'area': 49577.94434999999, 'iscrowd': 0, 'image_id': 9, 'bbox': [249.6, 229.27, 316.24, 245.08], 'category_id': 56, 'id': 1058555}, {'segmentation': [[434.48, 152.33, 433.51, 184.93, 425.44, 189.45, 376.7, 195.58, 266.94, 248.53, 179.78, 290.17, 51.62, 346.66, 16.43, 366.68, 1.9, 388.63, 0.0, 377.33, 0.0, 357.64, 0.0, 294.04, 22.56, 294.37, 56.14, 300.82, 83.58, 300.82, 109.08, 289.2, 175.26, 263.38, 216.9, 243.36, 326.34, 197.52, 387.03, 172.34, 381.54, 162.33, 380.89, 147.16, 380.89, 140.06, 370.89, 102.29, 330.86, 61.94, 318.91, 48.38, 298.57, 47.41, 287.28, 37.73, 259.51, 33.85, 240.14, 32.56, 240.14, 28.36, 247.57, 24.17, 271.46, 15.13, 282.11, 13.51, 296.96, 18.68, 336.34, 55.48, 391.55, 106.81, 432.87, 147.16], [62.46, 97.21, 130.25, 69.77, 161.25, 59.12, 183.52, 52.02, 180.94, 59.12, 170.93, 78.17, 170.28, 90.76, 157.05, 95.92, 130.25, 120.78, 119.92, 129.49, 102.17, 115.29, 64.72, 119.81, 0.0, 137.89, 0.0, 120.13, 0.0, 117.87]], 'area': 24292.781700000007, 'iscrowd': 0, 'image_id': 9, 'bbox': [0.0, 13.51, 434.48, 375.12], 'category_id': 51, 'id': 1534147}, {'segmentation': [[376.2, 61.55, 391.86, 46.35, 424.57, 40.36, 441.62, 43.59, 448.07, 50.04, 451.75, 63.86, 448.07, 68.93, 439.31, 70.31, 425.49, 73.53, 412.59, 75.38, 402.92, 84.13, 387.71, 86.89, 380.8, 70.77]], 'area': 2239.2924, 'iscrowd': 0, 'image_id': 9, 'bbox': [376.2, 40.36, 75.55, 46.53], 'category_id': 55, 'id': 1913551}, {'segmentation': [[473.92, 85.64, 469.58, 83.47, 465.78, 78.04, 466.87, 72.08, 472.84, 59.59, 478.26, 47.11, 496.71, 38.97, 514.62, 40.6, 521.13, 49.28, 523.85, 55.25, 520.05, 63.94, 501.06, 72.62, 482.6, 82.93]], 'area': 1658.8913000000007, 'iscrowd': 0, 'image_id': 9, 'bbox': [465.78, 38.97, 58.07, 46.67], 'category_id': 55, 'id': 1913746}, {'segmentation': [[385.7, 85.85, 407.12, 80.58, 419.31, 79.26, 426.56, 77.94, 435.45, 74.65, 442.7, 73.66, 449.95, 73.99, 456.87, 77.94, 463.46, 83.87, 467.74, 92.77, 469.39, 104.63, 469.72, 117.15, 469.39, 135.27, 468.73, 141.86, 466.09, 144.17, 449.29, 141.53, 437.1, 136.92, 430.18, 129.67]], 'area': 3609.3030499999995, 'iscrowd': 0, 'image_id': 9, 'bbox': [385.7, 73.66, 84.02, 70.51], 'category_id': 55, 'id': 1913856}, {'segmentation': [[458.81, 24.94, 437.61, 4.99, 391.48, 2.49, 364.05, 56.1, 377.77, 73.56, 377.77, 56.1, 392.73, 41.14, 403.95, 41.14, 420.16, 39.9, 435.12, 42.39, 442.6, 46.13, 455.06, 31.17]], 'area': 2975.276, 'iscrowd': 0, 'image_id': 9, 'bbox': [364.05, 2.49, 94.76, 71.07], 'category_id': 55, 'id': 1914001}]\n",
      "[{'segmentation': [[437.52, 353.33, 437.87, 326.98, 433.65, 306.26, 427.33, 287.29, 427.33, 279.56, 424.52, 266.21, 417.85, 255.68, 417.49, 248.65, 428.38, 234.95, 429.09, 223.71, 426.28, 211.77, 416.44, 192.45, 415.74, 191.05, 414.33, 182.27, 414.68, 180.51, 413.98, 170.32, 412.58, 168.22, 411.87, 165.41, 410.82, 149.6, 405.9, 121.15, 400.28, 107.45, 392.56, 108.85, 386.58, 101.13, 385.88, 99.72, 385.53, 92.7, 386.58, 88.48, 385.88, 84.62, 386.94, 81.1, 392.91, 68.11, 395.01, 60.03, 396.07, 60.03, 398.88, 65.65, 398.53, 68.46, 399.58, 70.57, 404.5, 68.81, 408.01, 61.79, 408.71, 61.08, 413.28, 62.84, 409.77, 73.38, 409.77, 77.24, 414.33, 81.81, 418.55, 80.75, 424.17, 77.59, 424.52, 80.4, 418.9, 86.72, 413.98, 93.05, 432.25, 127.82, 442.78, 157.68, 453.32, 172.43, 465.62, 179.46, 482.48, 198.42, 497.23, 214.23, 519.71, 226.17, 535.87, 251.81, 542.19, 268.67, 543.59, 272.89, 562.21, 285.18, 590.31, 293.96, 599.44, 297.12, 600.5, 321.01, 589.26, 318.9, 585.74, 313.98, 582.93, 304.5, 578.72, 296.77, 554.48, 284.83, 543.94, 279.21, 543.59, 311.88, 544.3, 325.22, 551.67, 343.14, 550.27, 347.71, 533.06, 347.35, 523.92, 347.71, 512.33, 348.76, 506.71, 350.16, 488.8, 305.2, 477.91, 295.72, 476.86, 294.67, 470.18, 339.98, 468.78, 355.08, 440.68, 356.49, 439.97, 357.19, 437.87, 357.19]], 'area': 19686.597949999996, 'iscrowd': 0, 'image_id': 25, 'bbox': [385.53, 60.03, 214.97, 297.16], 'category_id': 25, 'id': 598548}, {'segmentation': [[99.26, 405.72, 133.57, 393.78, 144.76, 381.1, 158.19, 369.91, 173.1, 388.56, 181.31, 390.8, 182.05, 390.8, 185.04, 381.85, 171.61, 360.22, 147.0, 356.49, 133.57, 366.18, 124.62, 373.64, 106.72, 378.12, 82.85, 382.59, 72.41, 390.8, 60.47, 399.0, 53.01, 410.19, 64.95, 411.68, 97.77, 406.46]], 'area': 2785.8475500000004, 'iscrowd': 0, 'image_id': 25, 'bbox': [53.01, 356.49, 132.03, 55.19], 'category_id': 25, 'id': 599491}]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5560\\1341765720.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5560\\1341765720.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5560\\1341765720.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    transform = MyTransform(transforms.Compose([\n",
    "                transforms.Resize((800,800)),\n",
    "                transforms.ToTensor(), \n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ]))\n",
    "\n",
    "\n",
    "    # Load the COCO dataset\n",
    "    train_dataset = CocoDetection(root='COCO/train2017',\n",
    "                                  annFile='COCO/annotations/instances_train2017.json', transforms=transform)\n",
    "    val_dataset = CocoDetection(root='COCO/val2017',\n",
    "                                annFile='COCO/annotations/instances_val2017.json', transforms=transform)\n",
    "\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=2, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=1, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Define the model\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Move model to gpu if available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Set the optimizer and the loss function\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Checkpoint saving path\n",
    "    ckpt_path = 'checkpoints/model_ckpt.pt'\n",
    "    start_epoch = 0\n",
    "    # Load the checkpoint if it exists\n",
    "    if os.path.exists(ckpt_path):\n",
    "        checkpoint = torch.load(ckpt_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        loss = checkpoint['loss']\n",
    "        print(f'Loaded checkpoint from epoch {start_epoch}')\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "\n",
    "    # Entrenar la red y guardar las pérdidas para la visualización\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    n_epochs_stop = 15\n",
    "\n",
    "    for epoch in range(start_epoch, 20000):\n",
    "        running_loss = 0.0\n",
    "        for i, (images, targets) in enumerate(train_loader):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            for t in targets:\n",
    "                print(t)\n",
    "            targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(images)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, targets)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 100 == 99 or i == len(train_loader) - 1:  # Asume que la indexación comienza en 0\n",
    "                avg_loss = running_loss / (i+1)\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, avg_loss))\n",
    "                train_losses.append(avg_loss)\n",
    "                writer.add_scalar('training loss', avg_loss, epoch * len(train_loader) + i)\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Validation loss\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                # This may not be the right way to calculate accuracy for object detection\n",
    "                # as it's more complicated than classification\n",
    "                predicted_boxes = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "                target_boxes = [{k: v.to('cpu') for k, v in t.items()} for t in targets]\n",
    "                total += len(target_boxes)\n",
    "                for predicted, target in zip(predicted_boxes, target_boxes):\n",
    "                    if predicted['boxes'].equal(target['boxes']):\n",
    "                        correct += 1\n",
    "\n",
    "        avg_val_loss = val_loss / total\n",
    "        val_accuracy = correct / total\n",
    "        print(f'Validation loss: {avg_val_loss:.3f}, Validation accuracy: {val_accuracy:.3f}')\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        writer.add_scalar('validation loss', avg_val_loss, epoch)\n",
    "        writer.add_scalar('validation accuracy', val_accuracy, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "            }, f'checkpoint{epoch}.pth')\n",
    "            print(f'Saved checkpoint at epoch {epoch}')\n",
    "\n",
    "        # Save the last checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "        }, 'latest_checkpoint.pth')\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print('Early stopping!')\n",
    "                model.load_state_dict(torch.load('best_model.pth'))\n",
    "                break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting the validation accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    writer.close()\n",
    "     \n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84946732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "one2",
   "language": "python",
   "name": "one2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
